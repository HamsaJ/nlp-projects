{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'models'\n",
      "creating pipeline\n",
      "adding labels\n",
      "get names of other pipes\n",
      "training a new model\n",
      "Losses {'ner': 4.807638990743499}\n",
      "Losses {'ner': 3.4844751060009003}\n",
      "Losses {'ner': 2.545009471476078}\n",
      "Losses {'ner': 3.929951224476099}\n",
      "Losses {'ner': 3.560788057744503}\n",
      "Losses {'ner': 3.98904275149107}\n",
      "Losses {'ner': 4.016806233674288}\n",
      "Losses {'ner': 2.817154921591282}\n",
      "Losses {'ner': 2.288880430161953}\n",
      "Losses {'ner': 2.941999234266719}\n",
      "Losses {'ner': 1.6972038322419394}\n",
      "Losses {'ner': 1.7995849028229713}\n",
      "Losses {'ner': 1.2077336050570011}\n",
      "Losses {'ner': 2.0141837115747068}\n",
      "Losses {'ner': 2.7251112684607506}\n",
      "Losses {'ner': 2.3611611500382423}\n",
      "Losses {'ner': 1.3638695368540852}\n",
      "Losses {'ner': 1.8124835267663002}\n",
      "Losses {'ner': 2.5978066101670265}\n",
      "Losses {'ner': 1.6401193737983704}\n",
      "Losses {'ner': 2.6338713839650154}\n",
      "Losses {'ner': 1.7399880141019821}\n",
      "Losses {'ner': 2.1653596349060535}\n",
      "Losses {'ner': 1.8020861595869064}\n",
      "Losses {'ner': 2.154963593930047}\n",
      "Losses {'ner': 1.0924126191011965}\n",
      "Losses {'ner': 2.1883318945765495}\n",
      "Losses {'ner': 2.0189733132719994}\n",
      "Losses {'ner': 1.7250554114580154}\n",
      "Losses {'ner': 1.3513659983873367}\n",
      "Losses {'ner': 1.4489581072680568}\n",
      "Losses {'ner': 1.402217485010624}\n",
      "Losses {'ner': 2.635416716337204}\n",
      "Losses {'ner': 0.8741407543420799}\n",
      "Losses {'ner': 1.834077425301075}\n",
      "Losses {'ner': 1.9562507309019566}\n",
      "Losses {'ner': 2.117590948939327}\n",
      "Losses {'ner': 1.8960380954325948}\n",
      "Losses {'ner': 1.4986486323177814}\n",
      "Losses {'ner': 1.7072917744517326}\n",
      "Losses {'ner': 1.9677084609866142}\n",
      "Losses {'ner': 1.6398607417941093}\n",
      "Losses {'ner': 2.14271130412817}\n",
      "Losses {'ner': 2.7151762768626213}\n",
      "Losses {'ner': 1.3506091982126236}\n",
      "Losses {'ner': 1.2416693493734894}\n",
      "Losses {'ner': 2.69862412288785}\n",
      "Losses {'ner': 1.6781250983476639}\n",
      "Losses {'ner': 1.8192138146041543}\n",
      "Losses {'ner': 1.9452382065355778}\n",
      "Losses {'ner': 1.932291753590107}\n",
      "Losses {'ner': 1.1331105306744576}\n",
      "Losses {'ner': 1.8854167610406876}\n",
      "Losses {'ner': 1.484375074505806}\n",
      "Losses {'ner': 1.5302232578396797}\n",
      "Losses {'ner': 1.332291729748249}\n",
      "Losses {'ner': 1.4375217258930206}\n",
      "Losses {'ner': 1.2972684763371944}\n",
      "Losses {'ner': 1.7373733818531036}\n",
      "Losses {'ner': 1.583782859146595}\n",
      "Losses {'ner': 1.9139956794679165}\n",
      "Losses {'ner': 2.0121643245220184}\n",
      "Losses {'ner': 1.670712660998106}\n",
      "Losses {'ner': 1.730208488155597}\n",
      "Losses {'ner': 1.2594318985939026}\n",
      "Losses {'ner': 1.3104166872799397}\n",
      "Losses {'ner': 2.289014980196953}\n",
      "Losses {'ner': 1.9187501035630703}\n",
      "Losses {'ner': 1.8166667968034744}\n",
      "Losses {'ner': 1.4483631551265717}\n",
      "Losses {'ner': 1.056250050663948}\n",
      "Losses {'ner': 1.21502978913486}\n",
      "Losses {'ner': 1.245833396911621}\n",
      "Losses {'ner': 1.464583482593298}\n",
      "Losses {'ner': 1.7093750983476639}\n",
      "Losses {'ner': 1.8219302520155907}\n",
      "Losses {'ner': 2.161806508898735}\n",
      "Losses {'ner': 2.1694360971458693}\n",
      "Losses {'ner': 1.9882441610097885}\n",
      "Losses {'ner': 1.4031250178813934}\n",
      "Losses {'ner': 1.1906250156462193}\n",
      "Losses {'ner': 1.5364584811031818}\n",
      "Losses {'ner': 1.406250063329935}\n",
      "Losses {'ner': 1.3339361473917961}\n",
      "Losses {'ner': 1.2343750409953298}\n",
      "Losses {'ner': 1.7218750566244125}\n",
      "Losses {'ner': 2.0016142018139362}\n",
      "Losses {'ner': 1.621875111013651}\n",
      "Losses {'ner': 1.9687500558793545}\n",
      "Losses {'ner': 1.8437500149011612}\n",
      "Losses {'ner': 1.3281250484287739}\n",
      "Losses {'ner': 1.2802084162831306}\n",
      "Losses {'ner': 1.8854193016886711}\n",
      "Losses {'ner': 0.9552083536982536}\n",
      "Losses {'ner': 1.7564640045166016}\n",
      "Losses {'ner': 1.7516830116510391}\n",
      "Losses {'ner': 1.4218750558793545}\n",
      "Losses {'ner': 1.3562500551342964}\n",
      "Losses {'ner': 1.5531250461935997}\n",
      "Losses {'ner': 1.460416741669178}\n",
      "Entities [('horses', 'ANIMAL')]\n",
      "Tokens [('horses', 'ANIMAL', 3), ('pretend', '', 2), ('to', '', 2), ('care', '', 2), ('about', '', 2), ('your', '', 2), ('feelings', '', 2)]\n",
      "Entities [('Horses', 'ANIMAL')]\n",
      "Tokens [('Horses', 'ANIMAL', 3), ('are', '', 2), ('too', '', 2), ('tall', '', 2), ('and', '', 2), ('they', '', 2), ('pretend', '', 2), ('to', '', 2), ('care', '', 2), ('about', '', 2), ('your', '', 2), ('feelings', '', 2)]\n",
      "Entities []\n",
      "Tokens [('Do', '', 2), ('they', '', 2), ('bite', '', 2), ('?', '', 2)]\n",
      "Entities [('horses', 'ANIMAL')]\n",
      "Tokens [('horses', 'ANIMAL', 3), ('are', '', 2), ('too', '', 2), ('tall', '', 2), ('and', '', 2), ('they', '', 2), ('pretend', '', 2), ('to', '', 2), ('care', '', 2), ('about', '', 2), ('your', '', 2), ('feelings', '', 2)]\n",
      "Entities [('horses', 'ANIMAL')]\n",
      "Tokens [('horses', 'ANIMAL', 3), ('?', '', 2)]\n",
      "Entities [('horses', 'ANIMAL')]\n",
      "Tokens [('they', '', 2), ('pretend', '', 2), ('to', '', 2), ('care', '', 2), ('about', '', 2), ('your', '', 2), ('feelings', '', 2), (',', '', 2), ('those', '', 2), ('horses', 'ANIMAL', 3)]\n",
      "Saved model to models\n",
      "Loading from models\n",
      "Entities [('horses', 'ANIMAL')]\n",
      "Tokens [('horses', 'ANIMAL', 3), ('pretend', '', 2), ('to', '', 2), ('care', '', 2), ('about', '', 2), ('your', '', 2), ('feelings', '', 2)]\n",
      "Entities [('Horses', 'ANIMAL')]\n",
      "Tokens [('Horses', 'ANIMAL', 3), ('are', '', 2), ('too', '', 2), ('tall', '', 2), ('and', '', 2), ('they', '', 2), ('pretend', '', 2), ('to', '', 2), ('care', '', 2), ('about', '', 2), ('your', '', 2), ('feelings', '', 2)]\n",
      "Entities []\n",
      "Tokens [('Do', '', 2), ('they', '', 2), ('bite', '', 2), ('?', '', 2)]\n",
      "Entities [('horses', 'ANIMAL')]\n",
      "Tokens [('horses', 'ANIMAL', 3), ('are', '', 2), ('too', '', 2), ('tall', '', 2), ('and', '', 2), ('they', '', 2), ('pretend', '', 2), ('to', '', 2), ('care', '', 2), ('about', '', 2), ('your', '', 2), ('feelings', '', 2)]\n",
      "Entities [('horses', 'ANIMAL')]\n",
      "Tokens [('horses', 'ANIMAL', 3), ('?', '', 2)]\n",
      "Entities [('horses', 'ANIMAL')]\n",
      "Tokens [('they', '', 2), ('pretend', '', 2), ('to', '', 2), ('care', '', 2), ('about', '', 2), ('your', '', 2), ('feelings', '', 2), (',', '', 2), ('those', '', 2), ('horses', 'ANIMAL', 3)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    (\"Horses are too tall and they pretend to care about your feelings\", {\n",
    "        'entities': [(0, 6, 'ANIMAL')]\n",
    "    }),\n",
    "\n",
    "    (\"Do they bite?\", {\n",
    "        'entities': []\n",
    "    }),\n",
    "\n",
    "    (\"horses are too tall and they pretend to care about your feelings\", {\n",
    "        'entities': [(0, 6, 'ANIMAL')]\n",
    "    }),\n",
    "\n",
    "    (\"horses pretend to care about your feelings\", {\n",
    "        'entities': [(0, 6, 'ANIMAL')]\n",
    "    }),\n",
    "\n",
    "    (\"they pretend to care about your feelings, those horses\", {\n",
    "        'entities': [(48, 54, 'ANIMAL')]\n",
    "    }),\n",
    "\n",
    "    (\"horses?\", {\n",
    "        'entities': [(0, 6, 'ANIMAL')]\n",
    "    })\n",
    "]\n",
    "def main(model='models', output_dir='models', n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    print(\"creating pipeline\")\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    print(\"adding labels\")\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    print(\"get names of other pipes\")\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        print(\"training a new model\")\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_simple = spacy.load('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_simple('I have heard Alex just bough a new horse for his sister.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ents in doc.ents:\n",
    "    print(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'check_flag',\n",
       " 'cluster',\n",
       " 'flags',\n",
       " 'from_bytes',\n",
       " 'has_vector',\n",
       " 'is_alpha',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'sentiment',\n",
       " 'set_attrs',\n",
       " 'set_flag',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'text',\n",
       " 'to_bytes',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
