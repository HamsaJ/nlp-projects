{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking candidate CVs using keywords and semantic matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 documents\n",
      "0  \t  Assistant Retail Manager  : \t Assistant Retail Manager    ROBERT SMITH    Phone: (123) 456 78 99 Email: info@qwikresume.com Websit\n",
      "1  \t  Python Developer/Tester  : \t     ROBERT SMITH    Python Developer/Tester    info@qwikresume.com | LinkedIn Profile | Qwikresume.c\n",
      "2  \t  SOFTWARE EXPERT  : \t Anthony Applicant    567 North Street  •  Boston, MA 02108  •  (123) 456-7890  •  anthony.applicant@\n",
      "3  \t  Full Stack Python Developer  : \t E­mail: info@qwikresumc.com    ROBERT SMITH Full Stack Python Developer    SUMMARY    Phone: (0123)­\n",
      "4  \t  Python Developer  : \t CONTACT DETAILS    1737 Marshville Road, Alabama    (123)-456-7899 info@qwikresume.com www.qwikresum\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load CVs\n",
    "with open('test_CVs.json') as in_file:\n",
    "    test_data = json.load(in_file)\n",
    "    \n",
    "# Load CVs\n",
    "with open('test_job_application.json') as in_file:\n",
    "    job_data = json.load(in_file)\n",
    "\n",
    "titles = [item[0] for item in test_data['data']]\n",
    "documents = [item[1] for item in test_data['data']]\n",
    "job_application = job_data['data'].split(\"#\")[1]\n",
    "\n",
    "print(f'{len(documents)} documents')\n",
    "\n",
    "for idx in range(5):\n",
    "    print(idx, \" \\t \", titles[idx], \" : \\t\", documents[idx][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>\n",
    "## 1. TF-IDF to score shared key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jama/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.111 \t Data Scientist\n",
      "0.098 \t Software Developer\n",
      "0.069 \t SOFTWARE EXPERT\n",
      "0.059 \t Python Developer\n",
      "0.045 \t Full Stack Python Developer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "vectors = vectorizer.fit_transform([job_application] + documents)\n",
    "\n",
    "# Calculate the word frequency, and calculate the cosine similarity of the search terms to the documents\n",
    "cosine_similarities = linear_kernel(vectors[0:1], vectors).flatten()\n",
    "document_scores = [item.item() for item in cosine_similarities[1:]]  # convert back to native Python dtypes\n",
    "\n",
    "# Print the top-scoring results and their titles\n",
    "score_titles = [(score, title) for score, title in zip(document_scores, titles)]\n",
    "\n",
    "for score, title in (sorted(score_titles, reverse=True, key=lambda x: x[0])[:5]):\n",
    "    print(f'{score:0.3f} \\t {title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1b\"></a>\n",
    "## 1b. Using a lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lemmatizer reduces words down to their simplest 'lemma'. This is particularly helpful with dealing with plurals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    \"\"\"\n",
    "    Interface to the WordNet lemmatizer from nltk\n",
    "    \"\"\"\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jama/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It', 'wa', 'raining', 'cat', 'and', 'dog', 'in', 'FooBar']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate the job of the tokenizer\n",
    "nltk.download('wordnet')\n",
    "tokenizer=LemmaTokenizer()\n",
    "\n",
    "tokenizer('It was raining cats and dogs in FooBar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.119 \t Data Scientist\n",
      "0.087 \t Software Developer\n",
      "0.068 \t SOFTWARE EXPERT\n",
      "0.040 \t Python Developer\n",
      "0.037 \t Full Stack Python Developer\n"
     ]
    }
   ],
   "source": [
    "# Initialise TfidfVectorizer with the LemmaTokenizer. Also need to lemmatize the stop words as well\n",
    "token_stop = tokenizer(' '.join(stop_words))\n",
    "vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer)\n",
    "\n",
    "# Calculate the word frequency, and calculate the cosine similarity of the search terms to the documents\n",
    "vectors = vectorizer.fit_transform([job_application] + documents)\n",
    "cosine_similarities = linear_kernel(vectors[0:1], vectors).flatten()\n",
    "\n",
    "document_scores = [item.item() for item in cosine_similarities[1:]]  # convert back to native Python dtypes\n",
    "\n",
    "score_titles = [(score, title) for score, title in zip(document_scores, titles)]\n",
    "\n",
    "for score, title in (sorted(score_titles, reverse=True, key=lambda x: x[0])[:5]):\n",
    "    print(f'{score:0.3f} \\t {title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1c\"></a>\n",
    "## 1c. Using the standalone module\n",
    "\n",
    "You can find the above functionality (TFidfVectorizer, stop_words, LemmaTokenizer, cosine_similarity) inside the `tfidf.py` module. This allows document scores to be calculated from a single function call: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jama/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tfidf import rank_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_scores = rank_documents(job_application, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.119 \t Data Scientist\n",
      "0.087 \t Software Developer\n",
      "0.068 \t SOFTWARE EXPERT\n",
      "0.040 \t Python Developer\n",
      "0.037 \t Full Stack Python Developer\n"
     ]
    }
   ],
   "source": [
    "score_titles = [(score, title) for score, title in zip(document_scores, titles)]\n",
    "\n",
    "for score, title in (sorted(score_titles, reverse=True, key=lambda x: x[0])[:5]):\n",
    "    print(f'{score:0.3f} \\t {title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>\n",
    "## 2. Semantic matching using GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from re import sub\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Initialize logging.\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)  # DEBUG # INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jama/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Import and download stopwords from NLTK.\n",
    "nltk.download('stopwords')  # Download stopwords list.\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support functions for pre-processing and calculation\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "\n",
    "def preprocess(doc):\n",
    "    # Tokenize, clean up input document string\n",
    "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
    "    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
    "    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
    "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
    "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the documents, including the query string\n",
    "corpus = [preprocess(document) for document in documents]\n",
    "query = preprocess(job_application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "The word embedding model is a large file, so loading is quite a long-running task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.3 s, sys: 197 ms, total: 21.5 s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Download and/or load the GloVe word vector embeddings\n",
    "\n",
    "if 'glove' not in locals():  # only load if not already in memory\n",
    "    glove = api.load(\"glove-wiki-gigaword-50\")\n",
    "    \n",
    "similarity_index = WordEmbeddingSimilarityIndex(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 s, sys: 286 ms, total: 23 s\n",
      "Wall time: 5.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build the term dictionary, TF-idf model\n",
    "# The search query must be in the dictionary as well, in case the terms do not overlap with the documents (we still want similarity)\n",
    "dictionary = Dictionary(corpus+[query])\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "\n",
    "# Create the term similarity matrix. \n",
    "# The nonzero_limit enforces sparsity by limiting the number of non-zero terms in each column. \n",
    "# For my application, I got best results by removing the default value of 100\n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)  # , nonzero_limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Soft Cosine Measure between the query and the documents.\n",
    "query_tf = tfidf[dictionary.doc2bow(query)]\n",
    "\n",
    "index = SoftCosineSimilarity(\n",
    "            tfidf[[dictionary.doc2bow(document) for document in corpus]],\n",
    "            similarity_matrix)\n",
    "\n",
    "doc_similarity_scores = index[query_tf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the document similarity results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \t 0.622 \t Data Scientist\n",
      "6 \t 0.548 \t Software Developer\n",
      "4 \t 0.529 \t Python Developer\n",
      "3 \t 0.521 \t Full Stack Python Developer\n",
      "1 \t 0.468 \t Python Developer/Tester\n",
      "0 \t 0.429 \t Assistant Retail Manager\n",
      "2 \t 0.371 \t SOFTWARE EXPERT\n"
     ]
    }
   ],
   "source": [
    "# Output the similarity scores for top 15 documents\n",
    "sorted_indexes = np.argsort(doc_similarity_scores)[::-1]\n",
    "for idx in sorted_indexes[:15]:\n",
    "    print(f'{idx} \\t {doc_similarity_scores[idx]:0.3f} \\t {titles[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most relevant terms in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each term in the search query, what were the most similar words in each document?\n",
    "doc_similar_terms = []\n",
    "max_results_per_doc = 5\n",
    "for term in query:\n",
    "    idx1 = dictionary.token2id[term]\n",
    "    for document in corpus:\n",
    "        results_this_doc = []\n",
    "        for word in set(document):\n",
    "            idx2 = dictionary.token2id[word]\n",
    "            score = similarity_matrix.matrix[idx1, idx2]\n",
    "            if score > 0.0:\n",
    "                results_this_doc.append((word, score))\n",
    "        results_this_doc = sorted(results_this_doc, reverse=True, key=lambda x: x[1])  # sort results by score\n",
    "        results_this_doc = results_this_doc[:min(len(results_this_doc), max_results_per_doc)]  # take the top results\n",
    "        doc_similar_terms.append(results_this_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \t 0.622 \t Data Scientist  :  passionate\n",
      "6 \t 0.548 \t Software Developer  :  \n",
      "4 \t 0.529 \t Python Developer  :  \n",
      "3 \t 0.521 \t Full Stack Python Developer  :  \n",
      "1 \t 0.468 \t Python Developer/Tester  :  \n",
      "0 \t 0.429 \t Assistant Retail Manager  :  \n",
      "2 \t 0.371 \t SOFTWARE EXPERT  :  hero\n"
     ]
    }
   ],
   "source": [
    "# Output the results for the top 15 documents\n",
    "for idx in sorted_indexes[:15]:\n",
    "    similar_terms_string = ', '.join([result[0] for result in doc_similar_terms[idx]])\n",
    "    print(f'{idx} \\t {doc_similarity_scores[idx]:0.3f} \\t {titles[idx]}  :  {similar_terms_string}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows which terms in each of the documents were most similar to terms in the search query. What it doesn't show, however, is the exact contribution of each of the terms to the document score, as each word similarity score will be weighted by the term frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2b\"></a>\n",
    "## 2b. Using the ready-made DocSim class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DocSim` class wraps up functionality to prepare and compare data in a single object. It also persists the word embedding model to avoid having to reload it each time it is used. The word embedding model is loaded on initialisation, as this is quite a long-running task.\n",
    "\n",
    "`DocSim_threaded` has similar functionality, but loads the model in a separate thread. Similarity queries cannot be evaluated until the model is ready - check the status of the `model_ready` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import docsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default GloVe word vector model: glove-wiki-gigaword-50\n",
      "Model loaded\n",
      "CPU times: user 20.7 s, sys: 169 ms, total: 20.9 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "docsim_obj = docsim.DocSim(verbose=True)\n",
    "# docsim_obj = docsim.DocSim_threaded(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready: True\n"
     ]
    }
   ],
   "source": [
    "print(f'Model ready: {docsim_obj.model_ready}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 documents loaded into corpus\n",
      "CPU times: user 21.4 s, sys: 202 ms, total: 21.6 s\n",
      "Wall time: 5.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "similarities = docsim_obj.similarity_query(job_application, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \t 0.622 \t Data Scientist\n",
      "6 \t 0.548 \t Software Developer\n",
      "4 \t 0.529 \t Python Developer\n",
      "3 \t 0.521 \t Full Stack Python Developer\n",
      "1 \t 0.468 \t Python Developer/Tester\n",
      "0 \t 0.429 \t Assistant Retail Manager\n",
      "2 \t 0.371 \t SOFTWARE EXPERT\n"
     ]
    }
   ],
   "source": [
    "# Output the similarity scores for top 15 documents\n",
    "for idx, score in (sorted(enumerate(similarities), reverse=True, key=lambda x: x[1])[:15]):\n",
    "    print(f'{idx} \\t {score:0.3f} \\t {titles[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
