{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide \n",
    "\n",
    "# to make the following output more readable I'll turn off the token sequence length warning\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "\n",
    "class DocumentReader:\n",
    "    def __init__(self, pretrained_model_name_or_path='bert-large-uncased'):\n",
    "        self.READER_PATH = pretrained_model_name_or_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)\n",
    "        self.max_len = self.model.config.max_position_embeddings\n",
    "        self.chunked = False\n",
    "\n",
    "    def tokenize(self, question, text):\n",
    "        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "        if len(self.input_ids) > self.max_len:\n",
    "            self.inputs = self.chunkify()\n",
    "            self.chunked = True\n",
    "\n",
    "    def chunkify(self):\n",
    "        \"\"\" \n",
    "        Break up a long article into chunks that fit within the max token\n",
    "        requirement for that Transformer model. \n",
    "\n",
    "        Calls to BERT / RoBERTa / ALBERT require the following format:\n",
    "        [CLS] question tokens [SEP] context tokens [SEP].\n",
    "        \"\"\"\n",
    "\n",
    "        # create question mask based on token_type_ids\n",
    "        # value is 0 for question tokens, 1 for context tokens\n",
    "        qmask = self.inputs['token_type_ids'].lt(1)\n",
    "        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n",
    "        chunk_size = self.max_len - qt.size()[0] - 1 # the \"-1\" accounts for\n",
    "        print(f\"Each chunk will contain {chunk_size - 2}\")\n",
    "        # having to add an ending [SEP] token to the end\n",
    "\n",
    "        # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
    "        chunked_input = OrderedDict()\n",
    "        for k,v in self.inputs.items():\n",
    "            q = torch.masked_select(v, qmask)\n",
    "            c = torch.masked_select(v, ~qmask)\n",
    "            chunks = torch.split(c, chunk_size)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if i not in chunked_input:\n",
    "                    chunked_input[i] = {}\n",
    "\n",
    "                thing = torch.cat((q, chunk))\n",
    "                if i != len(chunks)-1:\n",
    "                    if k == 'input_ids':\n",
    "                        thing = torch.cat((thing, torch.tensor([102])))\n",
    "                    else:\n",
    "                        thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n",
    "        return chunked_input\n",
    "\n",
    "    def get_answer(self):\n",
    "        if self.chunked:\n",
    "            answer = ''\n",
    "            for k, chunk in self.inputs.items():\n",
    "                answer_start_scores, answer_end_scores = self.model(**chunk)\n",
    "\n",
    "                answer_start = torch.argmax(answer_start_scores)\n",
    "                answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n",
    "                if ans != '[CLS]':\n",
    "                    answer += ans + \" / \"\n",
    "            return answer\n",
    "        else:\n",
    "            answer_start_scores, answer_end_scores = self.model(**self.inputs)\n",
    "\n",
    "            answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
    "            answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
    "        \n",
    "            return self.convert_ids_to_string(self.inputs['input_ids'][0][\n",
    "                                              answer_start:answer_end])\n",
    "\n",
    "    def convert_ids_to_string(self, input_ids):\n",
    "        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/jama/development/miniconda3/envs/nlp/lib/python3.8/site-packages (from wikipedia) (4.9.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/jama/development/miniconda3/envs/nlp/lib/python3.8/site-packages (from wikipedia) (2.24.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jama/development/miniconda3/envs/nlp/lib/python3.8/site-packages (from beautifulsoup4->wikipedia) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jama/development/miniconda3/envs/nlp/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/jama/development/miniconda3/envs/nlp/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/jama/development/miniconda3/envs/nlp/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/jama/development/miniconda3/envs/nlp/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.10)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11686 sha256=afceb0840f8da11817e502ee8ddc01b9efe973e07f009a8587cb06adc38f769d\n",
      "  Stored in directory: /Users/jama/Library/Caches/pip/wheels/07/93/05/72c05349177dca2e0ba31a33ba4f7907606f7ddef303517c6a\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When was Barack Obama born?\n",
      "Question: Why is the sky blue?\n",
      "Top wiki result: <WikipediaPage 'Diffuse sky radiation'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each chunk will contain 501\n",
      "Answer: Rayleigh scattering / \n",
      "\n",
      "Question: How many sides does a pentagon have?\n",
      "Top wiki result: <WikipediaPage 'The Pentagon'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each chunk will contain 497\n",
      "Answer: five / \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wikipedia as wiki\n",
    "import pprint as pp\n",
    "\n",
    "\n",
    "questions = [\n",
    "    'When was Barack Obama born?',\n",
    "    'Why is the sky blue?',\n",
    "    'How many sides does a pentagon have?'\n",
    "]\n",
    "\n",
    "reader = DocumentReader(\"deepset/bert-base-cased-squad2\") \n",
    "\n",
    "# if you trained your own model using the training cell earlier, you can access it with this:\n",
    "#reader = DocumentReader(\"./models/bert/bbu_squad2\")\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    try:\n",
    "        results = wiki.search(question)\n",
    "\n",
    "        page = wiki.page(results[0])\n",
    "        print(f\"Top wiki result: {page}\")\n",
    "\n",
    "        text = page.content\n",
    "\n",
    "        reader.tokenize(question, text)\n",
    "        print(f\"Answer: {reader.get_answer()}\")\n",
    "        print()\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia search results for our question:\n",
      "\n",
      "['Albatross',\n",
      " 'Black-browed albatross',\n",
      " 'List of largest birds',\n",
      " 'Mollymawk',\n",
      " 'List of birds by flight speed',\n",
      " 'Argentavis',\n",
      " 'Largest body part',\n",
      " 'Pterosaur',\n",
      " 'Pteranodon',\n",
      " 'Aspect ratio (aeronautics)']\n",
      "\n",
      "The Albatross Wikipedia article contains 38523 characters.\n"
     ]
    }
   ],
   "source": [
    "import wikipedia as wiki\n",
    "import pprint as pp\n",
    "\n",
    "question = 'What is the wingspan of an albatross?'\n",
    "\n",
    "results = wiki.search(question)\n",
    "print(\"Wikipedia search results for our question:\\n\")\n",
    "pp.pprint(results)\n",
    "\n",
    "page = wiki.page(results[0])\n",
    "text = page.content\n",
    "print(f\"\\nThe {results[0]} Wikipedia article contains {len(text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# executing these commands for the first time initiates a download of the \n",
    "# model weights to ~/.cache/torch/transformers/\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\") \n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This translates into 8887 tokens.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(question, text, return_tensors='pt')\n",
    "print(f\"This translates into {len(inputs['input_ids'][0])} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question consists of 12 tokens.\n",
      "Each chunk will contain 497 tokens of the Wikipedia article.\n"
     ]
    }
   ],
   "source": [
    "# time to chunk!\n",
    "from collections import OrderedDict\n",
    "\n",
    "# identify question tokens (token_type_ids = 0)\n",
    "qmask = inputs['token_type_ids'].lt(1)\n",
    "qt = torch.masked_select(inputs['input_ids'], qmask)\n",
    "print(f\"The question consists of {qt.size()[0]} tokens.\")\n",
    "\n",
    "chunk_size = model.config.max_position_embeddings - qt.size()[0] - 1 # the \"-1\" accounts for\n",
    "# having to add a [SEP] token to the end of each chunk\n",
    "print(f\"Each chunk will contain {chunk_size - 2} tokens of the Wikipedia article.\")\n",
    "\n",
    "# create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
    "chunked_input = OrderedDict()\n",
    "for k,v in inputs.items():\n",
    "    q = torch.masked_select(v, qmask)\n",
    "    c = torch.masked_select(v, ~qmask)\n",
    "    chunks = torch.split(c, chunk_size)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i not in chunked_input:\n",
    "            chunked_input[i] = {}\n",
    "\n",
    "        thing = torch.cat((q, chunk))\n",
    "        if i != len(chunks)-1:\n",
    "            if k == 'input_ids':\n",
    "                thing = torch.cat((thing, torch.tensor([102])))\n",
    "            else:\n",
    "                thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "        chunked_input[i][k] = torch.unsqueeze(thing, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
