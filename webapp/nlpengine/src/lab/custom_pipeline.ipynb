{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESTCountriesComponent(object):\n",
    "    \"\"\"spaCy v2.0 pipeline component that requests all countries via\n",
    "    the REST Countries API, merges country names into one token, assigns entity\n",
    "    labels and sets attributes on country tokens.\n",
    "    \"\"\"\n",
    "    name = 'rest_countries' # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp, label='GPE'):\n",
    "        \"\"\"Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the shared vocab, get the label ID and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \"\"\"\n",
    "        # Make request once on initialisation and store the data\n",
    "        r = requests.get('https://restcountries.eu/rest/v2/all')\n",
    "        r.raise_for_status()  # make sure requests raises an error if it fails\n",
    "        countries = r.json()\n",
    "\n",
    "        # Convert API response to dict keyed by country name for easy lookup\n",
    "        # This could also be extended using the alternative and foreign language\n",
    "        # names provided by the API\n",
    "        self.countries = {c['name']: c for c in countries}\n",
    "        self.label = nlp.vocab.strings[label]  # get entity label ID\n",
    "\n",
    "        # Set up the PhraseMatcher with Doc patterns for each country name\n",
    "        patterns = [nlp(c) for c in self.countries.keys()]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add('COUNTRIES', None, *patterns)\n",
    "\n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        # If no default value is set, it defaults to None.\n",
    "        Token.set_extension('is_country', default=False)\n",
    "        Token.set_extension('country_capital', default=False)\n",
    "        Token.set_extension('country_latlng', default=False)\n",
    "        Token.set_extension('country_flag', default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_country == True.\n",
    "        Doc.set_extension('has_country', getter=self.has_country)\n",
    "        Span.set_extension('has_country', getter=self.has_country)\n",
    "\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity & set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            # Can be extended with other data returned by the API, like\n",
    "            # currencies, country code, flag, calling code etc.\n",
    "            for token in entity:\n",
    "                token._.set('is_country', True)\n",
    "                token._.set('country_capital', self.countries[entity.text]['capital'])\n",
    "                token._.set('country_latlng', self.countries[entity.text]['latlng'])\n",
    "                token._.set('country_flag', self.countries[entity.text]['flag'])\n",
    "            # Overwrite doc.ents and add entity – be careful not to replace!\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token. This is done\n",
    "            # after setting the entities – otherwise, it would cause mismatched\n",
    "            # indices!\n",
    "            span.merge()\n",
    "        return doc  # don't forget to return the Doc!\n",
    "\n",
    "    def has_country(self, tokens):\n",
    "        \"\"\"Getter for Doc and Span attributes. Returns True if one of the tokens\n",
    "        is a country. Since the getter is only called when we access the\n",
    "        attribute, we can refer to the Token's 'is_country' attribute here,\n",
    "        which is already set in the processing step.\"\"\"\n",
    "        return any([t._.get('is_country') for t in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['print_info', 'tagger', 'parser', 'ner']\n",
      "After tokenization, this doc has 5 tokens.\n",
      "This is a pretty short document.\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import plac\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "\n",
    "def main():\n",
    "    # For simplicity, we start off with only the blank English Language class\n",
    "    # and no model or pre-defined pipeline loaded.\n",
    "    nlp = English()\n",
    "    rest_countries = RESTCountriesComponent(nlp)  # initialise component\n",
    "    nlp.add_pipe(rest_countries) # add it to the pipeline\n",
    "    doc = nlp(u\"Some text about Colombia and the Czech Republic\")\n",
    "    print('Pipeline', nlp.pipe_names)  # pipeline contains component name\n",
    "    print('Doc has countries', doc._.has_country)  # Doc contains countries\n",
    "    for token in doc:\n",
    "        if token._.is_country:\n",
    "            print(token.text, token._.country_capital, token._.country_latlng,\n",
    "                token._.country_flag)  # country data\n",
    "    print('Entities', [(e.text, e.label_) for e in doc.ents])  # entities\n",
    "\n",
    "\n",
    "class RESTCountriesComponent(object):\n",
    "    \"\"\"spaCy v2.0 pipeline component that requests all countries via\n",
    "    the REST Countries API, merges country names into one token, assigns entity\n",
    "    labels and sets attributes on country tokens.\n",
    "    \"\"\"\n",
    "    name = 'rest_countries' # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp, label='GPE'):\n",
    "        \"\"\"Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the shared vocab, get the label ID and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \"\"\"\n",
    "        # Make request once on initialisation and store the data\n",
    "        r = requests.get('https://restcountries.eu/rest/v2/all')\n",
    "        r.raise_for_status()  # make sure requests raises an error if it fails\n",
    "        countries = r.json()\n",
    "\n",
    "        # Convert API response to dict keyed by country name for easy lookup\n",
    "        # This could also be extended using the alternative and foreign language\n",
    "        # names provided by the API\n",
    "        self.countries = {c['name']: c for c in countries}\n",
    "        self.label = nlp.vocab.strings[label]  # get entity label ID\n",
    "\n",
    "        # Set up the PhraseMatcher with Doc patterns for each country name\n",
    "        patterns = [nlp(c) for c in self.countries.keys()]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add('COUNTRIES', None, *patterns)\n",
    "\n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        # If no default value is set, it defaults to None.\n",
    "        Token.set_extension('is_country', default=False)\n",
    "        Token.set_extension('country_capital', default=False)\n",
    "        Token.set_extension('country_latlng', default=False)\n",
    "        Token.set_extension('country_flag', default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_country == True.\n",
    "        Doc.set_extension('has_country', getter=self.has_country)\n",
    "        Span.set_extension('has_country', getter=self.has_country)\n",
    "\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity & set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            # Can be extended with other data returned by the API, like\n",
    "            # currencies, country code, flag, calling code etc.\n",
    "            for token in entity:\n",
    "                token._.set('is_country', True)\n",
    "                token._.set('country_capital', self.countries[entity.text]['capital'])\n",
    "                token._.set('country_latlng', self.countries[entity.text]['latlng'])\n",
    "                token._.set('country_flag', self.countries[entity.text]['flag'])\n",
    "            # Overwrite doc.ents and add entity – be careful not to replace!\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token. This is done\n",
    "            # after setting the entities – otherwise, it would cause mismatched\n",
    "            # indices!\n",
    "            span.merge()\n",
    "        return doc  # don't forget to return the Doc!\n",
    "\n",
    "    def has_country(self, tokens):\n",
    "        \"\"\"Getter for Doc and Span attributes. Returns True if one of the tokens\n",
    "        is a country. Since the getter is only called when we access the\n",
    "        attribute, we can refer to the Token's 'is_country' attribute here,\n",
    "        which is already set in the processing step.\"\"\"\n",
    "        return any([t._.get('is_country') for t in tokens])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plac.call(main)\n",
    "\n",
    "    # Expected output:\n",
    "    # Pipeline ['rest_countries']\n",
    "    # Doc has countries True\n",
    "    # Colombia Bogotá [4.0, -72.0] https://restcountries.eu/data/col.svg\n",
    "    # Czech Republic Prague [49.75, 15.5] https://restcountries.eu/data/cze.svg\n",
    "    # Entities [('Colombia', 'GPE'), ('Czech Republic', 'GPE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['print_info', 'tagger', 'parser', 'ner']\n",
      "After tokenization, this doc has 4 tokens.\n",
      "This is a pretty short document.\n",
      "I live in London\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names) # ['print_info', 'tagger', 'parser', 'ner']\n",
    "doc = nlp(u\"I live in London\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(London,)\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = u\"hello\"\n",
    "b = \"hello\"\n",
    "True if a == b else False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
