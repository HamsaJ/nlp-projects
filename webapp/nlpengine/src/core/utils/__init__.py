from spacy.tokenizer import Tokenizer

def num_of_tokens(nlp, text):
    tokenizer = Tokenizer(nlp.vocab)
    return len(tokenizer(text))

def num_of_sentences(nlp):
    sentence_list = []
    for sent in nlp.sents:
        sentence_list.append(sent)
    return sentence_list